---
title: "P8106_HW4"
author:
- "Naomi Simon-Kumar"
- ns3782
date: "04/11/2025"
output:
  pdf_document:
    latex_engine: xelatex
    toc: true
    toc_depth: 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Loading libraries

```{r libraries, message=FALSE, warning=FALSE}

# Load libraries
library(tidyverse)
library(tidymodels)
library(caret)
library(ggplot2)   
library(rpart)
library(rpart.plot)
library(ranger)
library(gbm)

```

# Part 1: Tree Based Models using College Data

## Partition into training and testing set

```{r}

# Read in dataset
college <- read.csv("College.csv")

# Remove NAs
college <- na.omit(college)

# Set seed for reproducibility
set.seed(299)

# Split data into training and testing data
data_split_college <- initial_split(college, prop = 0.8)

# Extract the training and test data, removing college ID column
training_data_college <- training(data_split_college) %>% select(-College)
testing_data_college <- testing(data_split_college) %>% select(-College)


```


## a) Build a regression tree on training data

In order to implement the CART approach implementing recursive partitioning and pruning, I first fit a regression tree using cp=0 (complexity parameter). This parameter controls the complexity pruning in the CART algorithm, i.e., how splits are undertaken. Setting cp=0 was a safe choice to ensure that the tree was sufficiently large, allowing all potential splits are considered. I also produced a plot of this tree.

```{r}

# Set seed for reproducibility
set.seed(299)

# Fit initial tree: 
initial.tree.fit <- rpart(Outstate ~ ., data = training_data_college,
                      control = rpart.control(cp = 0))

# Tree plot
rpart.plot(initial.tree.fit)

# Print and plot the cp table
printcp(initial.tree.fit)
plotcp(initial.tree.fit)

```

To explore the impact of adjusting the complexity parameter, I also fit a second model setting cp=0.01. This model had fewer splits by comparison. When plotted, this tree was noticeably smaller than the previous tree.

```{r}

# Set seed for reproducibility
set.seed(299)

# Fit another tree: 
tree.fit.2 <- rpart(Outstate ~ ., data = training_data_college,
                      control = rpart.control(cp = 0.01))

# Tree plot
rpart.plot(tree.fit.2)

# Print and plot the cp table
printcp(tree.fit.2)
plotcp(tree.fit.2)

```

Importantly, setting cp=0 is the preferred choice, as it allowed for a large enough tree to be grown for the cost complexity table. In the cp = 0.01 model, the smallest xerror (scaled cross-validation error) was 0.37369, whereas the cp = 0 model achieved a slightly lower minimum xerror of 0.36867.
This shows us that a fully grown tree in this case is better suited for selecting an optimal complexity parameter based on cross-validation.

The optimal tree selected from the cp = 0 model has a **complexity parameter of 0.00770392**, with **11 splits** (i.e., 12 terminal nodes). This was the model that minimised scaled cross-validation error (**xerror = 0.36867**). Therefore, this was chosen as the final pruned tree.

## b) Perform random forest on training data

I decided to explore two tuning grids to find the optimal random forest model using cross-validation RMSE. The mtry parameter controls the number of predictors randomly selected at each split in the forest. I decided to tune mtry over the full range of possible values, from 1 to 16 (i.e., the full number of predictors in the college dataset). 

For the first grid (min.node.size = 1:7), the best model had mtry = 7 and min.node.size = 5. 

```{r}

# Set seed for reproducibility
set.seed(299)

# Set cross-validation
ctrl <- trainControl(method = "cv")

# Define grid for tuning mtry and min.node.size
rf.grid <- expand.grid(
  mtry = 1:16, # max no. of predictors
  splitrule = "variance",
  min.node.size = c(1:7)
)

# Fit random forest using ranger via caret
rf.fit <- train(
  Outstate ~ .,
  data = training_data_college,
  method = "ranger",
  tuneGrid = rf.grid,
  trControl = ctrl
)

# Obtain optimal tuning parameters from cross-validation
rf.fit$bestTune # mtry = 7, min.node.size = 5

ggplot(rf.fit, highlight = TRUE)


```

I then decided to try an expanded grid range for min.node.size, extending it to 1:10 to check for any better performing values beyond the original range.

```{r}

# Set seed for reproducibility
set.seed(299)

# Define another grid for tuning mtry and min.node.size
rf.grid2 <- expand.grid(
  mtry = 1:16, # max no. of predictors
  splitrule = "variance",
  min.node.size = c(1:10)
)

# Fit random forest using ranger via caret
rf.fit2 <- train(
  Outstate ~ .,
  data = training_data_college,
  method = "ranger",
  tuneGrid = rf.grid2,
  trControl = ctrl
)

# Optimal parameters
rf.fit2$bestTune  # mtry = 9, min.node.size = 3 (row 83)
rf.fit2$results[83, ] # pulling the lowest RMSE: 1763.865

# Plot performance for tuning grid values
ggplot(rf.fit2, highlight = TRUE)

```

From this grid, **the optimal tuning parameters were mtry = 9 and min.node.size = 3**, selected based on the lowest cross validation RMSE (1763.865). The corresponding plot shows the lowest RMSE.

Next, reporting the variable importance and the test error for this selected model (rf.fit2):

I first examined permutation-based variable importance in the dataset.

```{r}

# Set seed for reproducibility
set.seed(299)

# Fit random forest using ranger via caret
rf.fit2.per <- ranger(
  Outstate ~ .,
  data = training_data_college,
  mtry = rf.fit2$bestTune$mtry, # selecting the cross-validated parameter
  min.node.size = rf.fit2$bestTune$min.node.size, # selecting the cross-validated parameter
  splitrule = "variance",
  importance = "permutation", # selecting permutation
  scale.permutation.importance = TRUE
)

# Plotting the variable importance 
barplot(sort(ranger::importance(rf.fit2.per), decreasing = FALSE),
las = 2, horiz = TRUE, cex.names = 0.7,
col = colorRampPalette(colors = c("cyan","blue"))(19))

```

Based on this plot, **Expend** appears to have the highest variable importance, followed by **Room Board** and **Apps**, respectively. Books appears to be the least important variable, by comparison, followed by S.F. Ratio and P.Undergrad, in that order.

I decided to explore variable importance using the impurity method for the same model.

```{r}

# Set seed for reproducibility
set.seed(299)

# Fit random forest using ranger via caret
rf.fit2.imp <- ranger(
  Outstate ~ .,
  data = training_data_college,
  mtry = rf.fit2$bestTune$mtry, # selecting the cross-validated parameter
  min.node.size = rf.fit2$bestTune$min.node.size, # selecting the cross-validated parameter
  splitrule = "variance",
  importance = "impurity", # selecting impurity
  scale.permutation.importance = TRUE
)

# Plotting the variable importance 
barplot(sort(ranger::importance(rf.fit2.imp), decreasing = FALSE),
las = 2, horiz = TRUE, cex.names = 0.7,
col = colorRampPalette(colors = c("cyan","blue"))(19))

```

As with the permutation method, the most important variable based on this plot appears to be **Expend**, followed by Room.Board. However, the impurity method shows that PhD is the third most important variable, which is different to what was identified using the permutation method (the permutation method identified the third most important variable as Apps).

Books appears to be the least important variable using the impurity method, similar to the permutation method. However, unlike the permutation method, the subsequent least important variables are Enroll and F.Undergrad, in that order.

Next, finding the **test error of the model**: 

```{r}

# Set seed for reproducibility
set.seed(299)

# Predict on the test data
pred.rf2 <- predict(rf.fit2, newdata = testing_data_college)

# RMSE based on the test data
rf_test_rmse <- RMSE(pred.rf2, testing_data_college$Outstate)

# Test error
rf_test_rmse

```

Therefore, the **test error (RMSE) is 1629.167**.

## c) Perform boosting on training data

I proceeded to fit a gradient boosting model with the Gaussian loss function.

```{r}

# Set seed for reproducibility
set.seed(299)

# Initial grid for tuning
# gbm.grid <- expand.grid(
 # n.trees = c(100, 200, 500, 1000, 2000, 5000, 10000),
#  interaction.depth = 1:3, # we want to learn slowly, so keep small
#  shrinkage = c(0.005, 0.01, 0.05), # range of lambda values
# n.minobsinnode = 10  # Based on notes, this can be fixed
#)

# Fit the GBM model
# gbm.fit <- train(
 # Outstate ~ .,
#  data = training_data_college,
#  method = "gbm",
#  tuneGrid = gbm.grid,
 # trControl = ctrl,
#  verbose = FALSE
#)

# View best tuning parameters
# gbm.fit$bestTune 
# n trees = 2000; interaction.depth=3; shrinkage=0.005; n.minobsinnode=10


# Define grid for tuning
gbm.grid <- expand.grid(
  n.trees = c(100, 200, 500, 1000, 2000, 5000, 10000),
  interaction.depth = 1:4, # increase to 4, still keeping small
  shrinkage = c(0.001, 0.005, 0.01, 0.05), # adding an additional shrinkage
  n.minobsinnode = 10  
)

# Fit the GBM model
gbm.fit <- train(
  Outstate ~ .,
  data = training_data_college,
  method = "gbm",
  tuneGrid = gbm.grid,
  trControl = ctrl,
  verbose = FALSE
)

# View best tuning parameters
gbm.fit$bestTune # n.trees = 500; shrinkage=0.01, interaction.depth=4

# Plot the CV results
ggplot(gbm.fit, highlight = TRUE)


```

I initially tested a smaller grid when tuning the model parameters with interaction.depth = 1:3. The best model had interaction.depth = 3, which was the upper limit of that range. Even though it is acceptable for interaction.depth to be at the boundary, I decided to expand the grid to 1:4 to explore other ranges and to also explore another shrinkage parameter.

Based on the cross-validation RMSE plot showing the combinations of shrinkage (i.e., the learning rate), n.trees (number of boosting iterations), and interaction depth (tree depth), we can see that the lowest RMSE was achieved with shrinkage = 0.01, interaction.depth = 4, and approximately 500 boosting iterations (n.trees). This is consistent with the Generalized Boosting Models package documentation, which notes that the relationship between shrinkage and optimal iterations is roughly proportional, as our optimal n.trees decreased when we increased the shrinkage value (Ridgeway, 2024). 

Next, finding the **variable importance**.

```{r}

# Set seed for reproducibility
set.seed(299)

# Using summary to find variable importance for boosting
summary(gbm.fit$finalModel, las = 2, cBars = 19, cex.names = 0.6)

```

From this, we can see the most important predictor appears to be **Expend**, followed by Room.Board and perc.alumni, respectively.
The least important variable based on this summary of the boosting model appears to be Enroll, followed by P.Undergrad and S.F. Ratio, in that order.

Next, finding the **test error** for the boosting model:

```{r}

# Set seed for reproducibility
set.seed(299)

# Predict on the test data
pred.gbm.fit <- predict(gbm.fit, newdata = testing_data_college)

# RMSE based on the test data
gbm_test_rmse <- RMSE(pred.gbm.fit, testing_data_college$Outstate)

# Test error
gbm_test_rmse

```

The test error for the model is **1568.585**. 


# Part 2: Classification using Auto dataset

## Partition into training and testing set

```{r}


```


# References

Ridgeway, G. (2024, June 26). Generalized Boosted Models: A guide to the gbm package.RetryClaude can make mistakes. Please double-check responses.

