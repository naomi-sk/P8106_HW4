---
title: "P8106_HW4"
author:
- "Naomi Simon-Kumar"
- ns3782
date: "04/11/2025"
output:
  pdf_document:
    latex_engine: xelatex
    toc: true
    toc_depth: 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Loading libraries

```{r libraries, message=FALSE, warning=FALSE}

# Load libraries
library(tidyverse)
library(tidymodels)
library(caret)
library(ggplot2)   
library(rpart)
library(rpart.plot)
library(ranger)
library(gbm)
library(rsample)
library(pROC)

```

# Part 1: Tree Based Models using College Data

## Partition into training and testing set

```{r}

# Read in dataset
college <- read.csv("College.csv")

# Remove NAs
college <- na.omit(college)

# Set seed for reproducibility
set.seed(299)

# Split data into training and testing data
data_split_college <- initial_split(college, prop = 0.8)

# Extract the training and test data, removing college ID column
training_data_college <- training(data_split_college) %>% select(-College)
testing_data_college <- testing(data_split_college) %>% select(-College)



```


## a) Build a regression tree on training data

In order to implement the CART approach implementing recursive partitioning and pruning, I first fit a regression tree using cp=0 (complexity parameter). This parameter controls the complexity pruning in the CART algorithm, i.e., how splits are undertaken. Setting cp=0 was a safe choice to ensure that the tree was sufficiently large, allowing all potential splits are considered. I also produced a plot of this tree.

```{r}

# Set seed for reproducibility
set.seed(299)

# Fit initial tree: 
initial.tree.fit <- rpart(Outstate ~ ., 
                          data = training_data_college,
                          control = rpart.control(cp = 0))

# Tree plot
rpart.plot(initial.tree.fit)

# Print and plot the cp table
printcp(initial.tree.fit)
plotcp(initial.tree.fit)

```

To explore the impact of adjusting the complexity parameter, I also fit a second model setting cp=0.01. This model had fewer splits by comparison. When plotted, this tree was noticeably smaller than the previous tree.

```{r}

# Set seed for reproducibility
set.seed(299)

# Fit another tree: 
tree.fit.2 <- rpart(Outstate ~ ., 
                    data = training_data_college,
                    control = rpart.control(cp = 0.01))

# Tree plot
rpart.plot(tree.fit.2)

# Print and plot the cp table
printcp(tree.fit.2)
plotcp(tree.fit.2)

```

Importantly, setting cp=0 is the preferred choice, as it allowed for a large enough tree to be grown for the cost complexity table. In the cp = 0.01 model I tried, the smallest xerror (scaled cross-validation error) was 0.37369, whereas the cp = 0 model achieved a slightly lower minimum xerror of 0.36867. This shows us that a fully grown tree in this case is better suited for selecting an optimal complexity parameter based on cross-validation.

Therefore, we will proceed with the optimal tree selected from the **cp = 0 model** that has a **complexity parameter of 0.00770392**, with **11 splits** (i.e., 12 terminal nodes). This was the model that minimised scaled cross-validation error (**xerror = 0.36867**), as per the initial cp table. Therefore, this was chosen as the final pruned tree.

## b) Perform random forest on training data

I decided to explore two tuning grids to find the optimal random forest model using cross-validation RMSE. The mtry parameter controls the number of predictors randomly selected at each split in the forest. I decided to tune mtry over the full range of possible values, from 1 to 16 (i.e., the full number of predictors in the college dataset). 

For the first grid (min.node.size = 1:7), the best model had mtry = 7 and min.node.size = 5. 

```{r}

# Set seed for reproducibility
set.seed(299)

# Set cross-validation
ctrl <- trainControl(method = "cv", 
                     summaryFunction = defaultSummary)

# Define grid for tuning mtry and min.node.size
rf.grid <- expand.grid(
  mtry = 1:16, # max no. of predictors
  splitrule = "variance",
  min.node.size = c(1:7)
)

# Fit random forest using ranger via caret
rf.fit <- train(
  Outstate ~ .,
  data = training_data_college,
  method = "ranger",
  tuneGrid = rf.grid,
  trControl = ctrl
)

# Obtain optimal tuning parameters from cross-validation
rf.fit$bestTune # mtry = 7, min.node.size = 5

ggplot(rf.fit, highlight = TRUE)


```

I then decided to try an expanded grid range for min.node.size, extending it to 1:10 to check for any better performing values beyond the original range.

```{r}

# Set seed for reproducibility
set.seed(299)

# Define another grid for tuning mtry and min.node.size
rf.grid2 <- expand.grid(
  mtry = 1:16, # max no. of predictors
  splitrule = "variance",
  min.node.size = c(1:10)
)

# Fit random forest using ranger via caret
rf.fit2 <- train(
  Outstate ~ .,
  data = training_data_college,
  method = "ranger",
  tuneGrid = rf.grid2,
  trControl = ctrl
)

# Optimal parameters
rf.fit2$bestTune  # mtry = 9, min.node.size = 3 (row 83)
rf.fit2$results[83, ] # pulling the lowest RMSE: 1763.865

# Plot performance for tuning grid values
ggplot(rf.fit2, highlight = TRUE)

```

From this grid, **the optimal tuning parameters were mtry = 9 and min.node.size = 3**, selected based on the lowest cross validation RMSE (1763.865). The corresponding plot shows the lowest RMSE.

Next, reporting the variable importance and the test error for this selected model (rf.fit2):

I first examined **permutation-based variable importance** in the dataset.

```{r}

# Set seed for reproducibility
set.seed(299)

# Fit random forest using ranger via caret
rf.fit2.per <- ranger(
  Outstate ~ .,
  data = training_data_college,
  mtry = rf.fit2$bestTune$mtry, # selecting the cross-validated parameter
  min.node.size = rf.fit2$bestTune$min.node.size, # selecting the cross-validated parameter
  splitrule = "variance",
  importance = "permutation", # selecting permutation
  scale.permutation.importance = TRUE
)

# Plotting the variable importance 
barplot(sort(ranger::importance(rf.fit2.per), decreasing = FALSE),
las = 2, horiz = TRUE, cex.names = 0.7,
col = colorRampPalette(colors = c("cyan","blue"))(19))

```

Based on this plot, **Expend** appears to have the highest variable importance, followed by **Room Board** and **Apps**, respectively. Books appears to be the least important variable, by comparison, followed by S.F. Ratio and P.Undergrad, in that order.

I decided to explore variable importance using the impurity method for the same model.

```{r}

# Set seed for reproducibility
set.seed(299)

# Fit random forest using ranger via caret
rf.fit2.imp <- ranger(
  Outstate ~ .,
  data = training_data_college,
  mtry = rf.fit2$bestTune$mtry, # selecting the cross-validated parameter
  min.node.size = rf.fit2$bestTune$min.node.size, # selecting the cross-validated parameter
  splitrule = "variance",
  importance = "impurity", # selecting impurity
  scale.permutation.importance = TRUE
)

# Plotting the variable importance 
barplot(sort(ranger::importance(rf.fit2.imp), decreasing = FALSE),
las = 2, horiz = TRUE, cex.names = 0.7,
col = colorRampPalette(colors = c("cyan","blue"))(19))

```

As with the permutation method, the most important variable based on this plot appears to be **Expend**, followed by Room.Board. However, the impurity method shows that PhD is the third most important variable, which is different to what was identified using the permutation method (the permutation method identified the third most important variable as Apps).

Books appears to be the least important variable using the impurity method, similar to the permutation method. However, unlike the permutation method, the subsequent least important variables are Enroll and F.Undergrad, in that order. This difference is consistent with the literature, with permutation importance avoiding biases that affect impurity measures, though impurity based variable importance rankings can be more computationally efficient and sometimes more robust to data perturbations in high-dimensional settings (Nembrini, et al., 2018).

Next, finding the **test error of the model**: 

```{r}

# Set seed for reproducibility
set.seed(299)

# Predict on the test data
pred.rf2 <- predict(rf.fit2, newdata = testing_data_college)

# RMSE based on the test data
rf_test_rmse <- RMSE(pred.rf2, testing_data_college$Outstate)

# Test error
rf_test_rmse

```

Therefore, the **test error (RMSE) is 1629.167**.

## c) Perform boosting on training data

I proceeded to fit a gradient boosting model with the Gaussian loss function.

```{r}

# Set seed for reproducibility
set.seed(299)

# Initial grid for tuning
# gbm.grid <- expand.grid(
 # n.trees = c(100, 200, 500, 1000, 2000, 5000, 10000),
#  interaction.depth = 1:3, # we want to learn slowly, so keep small
#  shrinkage = c(0.005, 0.01, 0.05), # range of lambda values
# n.minobsinnode = 10  # Based on notes, this can be fixed
#)

# Fit the GBM model
# gbm.fit <- train(
 # Outstate ~ .,
#  data = training_data_college,
#  method = "gbm",
#  tuneGrid = gbm.grid,
 # trControl = ctrl,
#  verbose = FALSE
#)

# View best tuning parameters
# gbm.fit$bestTune 
# n trees = 2000; interaction.depth=3; shrinkage=0.005; n.minobsinnode=10


# Define grid for tuning
gbm.grid <- expand.grid(
  n.trees = c(100, 200, 500, 1000, 2000, 5000, 10000),
  interaction.depth = 1:4, # increase to 4, still keeping small
  shrinkage = c(0.001, 0.005, 0.01, 0.05), # adding an additional shrinkage
  n.minobsinnode = 10  
)

# Fit the GBM model
gbm.fit <- train(
  Outstate ~ .,
  data = training_data_college,
  method = "gbm",
  tuneGrid = gbm.grid,
  trControl = ctrl,
  verbose = FALSE
)

# View best tuning parameters
gbm.fit$bestTune # n.trees = 500; shrinkage=0.01, interaction.depth=4

# Plot the CV results
ggplot(gbm.fit, highlight = TRUE)


```

I initially tested a smaller grid when tuning the model parameters with interaction.depth = 1:3. The best model had interaction.depth = 3, which was the upper limit of that range. Even though it is acceptable for interaction.depth to be at the boundary, I decided to expand the grid to 1:4 to explore other ranges and to also explore another shrinkage parameter.

Based on the cross-validation RMSE plot showing the combinations of shrinkage (i.e., the learning rate), n.trees (number of boosting iterations), and interaction depth (tree depth), we can see that the lowest RMSE was achieved with **shrinkage = 0.01, n.minobsinnode = 10, interaction.depth = 4**, and approximately **500 boosting iterations (n.trees)**.  This is consistent with the Generalized Boosting Models package documentation, which notes that the relationship between shrinkage and optimal iterations is roughly proportional, as our optimal n.trees decreased when we increased the shrinkage value (Ridgeway, 2024). Notably, while n.minobsinnode was at the boundary of the range, it was noted by the Professor that this is acceptable.

Next, finding the **variable importance**.

```{r}

# Set seed for reproducibility
set.seed(299)

# Using summary to find variable importance for boosting
summary(gbm.fit$finalModel, las = 2, cBars = 19, cex.names = 0.6)

```

From this, we can see the most important predictor appears to be **Expend**, followed by Room.Board and perc.alumni, respectively.
The least important variable based on this summary of the boosting model appears to be Enroll, followed by P.Undergrad and S.F. Ratio, in that order.

Next, finding the **test error** for the boosting model:

```{r}

# Set seed for reproducibility
set.seed(299)

# Predict on the test data
pred.gbm.fit <- predict(gbm.fit, newdata = testing_data_college)

# RMSE based on the test data
gbm_test_rmse <- RMSE(pred.gbm.fit, testing_data_college$Outstate)

# Test error
gbm_test_rmse

```

The test error for the model is **1568.585**. 


# Part 2: Classification using Auto dataset

## Partition into training and testing set

```{r}

# Read in dataset
auto <- read.csv("auto.csv")

# Remove NAs
auto <- na.omit(auto)

# Make sure factor variables are correctly coded
auto$cylinders <- factor(auto$cylinders)
auto$origin <- factor(auto$origin)
auto$mpg_cat <- factor(auto$mpg_cat, levels = c("low", "high"))

# Check variable types
str(auto)
levels(auto$mpg_cat)

# Set seed for reproducibility
set.seed(299)

# Split data into training and testing data
data_split_auto <- initial_split(auto, prop = 0.7)


# Extract the training and test data
training_data_auto <- training(data_split_auto)
testing_data_auto <- testing(data_split_auto)

# Check variable types
# str(training_data_auto)
# str(testing_data_auto)

```

I made sure to recode the variables origin and cylinders to factor variable type. Although cylinders was originally represented as an integer, it is a multi-valued discrete variable as its values represent categorical groupings of engine types (i.e., 4, 6, 8 cylinder),

## a) Classification Tree

Using the rpart method, I proceeded to fit the tree:

```{r}

# Set seed for reproducibility
set.seed(299)

# Build tree
rpart.class.tree.fit <- rpart(formula = mpg_cat ~ . ,
               data = training_data_auto,
               control = rpart.control(cp = 0))

# Produce cp table and plot tree
cpTable <- printcp(rpart.class.tree.fit)
plotcp(rpart.class.tree.fit)

# Obtain cp with minimum xerror
minErr <- which.min(cpTable[, "xerror"])

# Prune the tree using best cp 
pruned.tree <- prune(rpart.class.tree.fit, cp = cpTable[minErr, "CP"])

# Plot pruned tree
rpart.plot(pruned.tree)

```

I set cp=0, to allow for a sufficiently large enough tree to be grown. From the results, two trees had the same minimum (scaled) cross-validation error (**xerror = 0.19549**). I chose the simpler model **with 1 split** to prioritise interpretability.

Therefore, the optimal tree selected from this cp = 0 model has a **complexity parameter of 0.0150376**, with **1 split** (i.e, **tree size corresponds to 2 terminal nodes** in the decision tree, as seen in the plot). This was the model that minimised scaled cross-validation error (**xerror = 0.19549**). Therefore, this was chosen as the final pruned tree.

Next, using the 1SE rule to build the tree:

```{r}

# Set seed for reproducibility
set.seed(299)

# Get min cross validation error and its standard error
min_xerror <- min(cpTable[, "xerror"])
xstd_at_min <- cpTable[which.min(cpTable[, "xerror"]), "xstd"]

# Get largest cp  so  xerror less or equal to min_xerror + xstd
cp_under_1se <- cpTable[cpTable[, "xerror"] <= min_xerror + xstd_at_min, ]

cp_under_1se

# 1st is largest cp 
cp_1se <- cp_under_1se[1, "CP"]  

# Use 1SE cp to prune tree
tree.fit.1se <- rpart::prune(rpart.class.tree.fit, cp = cp_1se)

# Plot tree
rpart.plot(tree.fit.1se)

```

In applying the 1se rule, we are looking for the smallest tree below the dotted line (i.e., representing minimum cross-validation error + 1 standard error). The 1se rule allows us to select the smallest tree whose cross-validation error is within one standard error of the minimum accordingly.
In this case, the **selected tree has 1 split, with 2 terminal nodes** (cp=0.015037594, xerror=0.1954887). Therefore, the tree selected by the 1SE rule is the same as the one selected by the minimum cross-validation error, and they have the same tree size. 

## b) Boosting model: AdaBoost

```{r}

# Set seed for reproducibility
set.seed(299)

# Check factor levels
levels(training_data_auto$mpg_cat)

# Set cross validation 
ctrl.adaboost <- trainControl(method = "cv",
                              classProbs = TRUE,
                              summaryFunction = twoClassSummary)

# Tuning grid for adaboost
gbm.ada.grid <- expand.grid(
  n.trees = c(100, 200, 500, 1000, 2000, 5000, 10000),
  interaction.depth = 1:4, # want to learn slowly, so keep small
  shrinkage = c(0.001, 0.003, 0.005, 0.01), # range of lambda values
  n.minobsinnode = 10 # based on notes, can fix this at 10
)

# Fit gbm with caret, using adaboost
gbm.ada.fit <- train(mpg_cat ~ .,
  data = training_data_auto,
  tuneGrid = gbm.ada.grid,
  trControl = ctrl.adaboost,
  method = "gbm",
  distribution = "adaboost",
  metric = "ROC",
  verbose = FALSE
)

# Plot to show best tuning parameters
ggplot(gbm.ada.fit, highlight = TRUE)

# Optimal tuning parameters
gbm.ada.fit$bestTune



```

I ensured the outcome variable mpg_cat was coded so that high was the second factor level, in order for it to be treated as the positive class when computing AUC using caret.

I then proceeded to perform boosting using adaboost. I initially tried a range of values for interaction depth between 1:5, however, this produced a shrinkage value at the boundary of the grid (0.01).
I then tuned the grid keeping interaction.depth at 4, which was appropriate as it gave me n.trees and shrinkage values not at the boundary of their respective grids. Based on this model, the optimal tuning parameters were **n.trees = 1000, interaction.depth = 4, shrinkage = 0.003, and n.minobsinnode = 10**, based on 10 fold cross validation AUC. This is evident based on the plot, where we can see the best performance at shrinkage=0.003 and interaction.depth=4. 

```{r}

# Set seed for reproducibility
set.seed(299)

# Presenting variable importance
summary(gbm.ada.fit$finalModel, las = 2, cBars = 19, cex.names = 0.6)

```

From this, we can see the most important predictor appears to be **displacement**, followed by **cylinders4** and **weight**, respectively. The least important variable based on this model appears to be **cylinders8**, followed by cylinders5 and origin3, in that order.

Next, obtaining test error:

```{r}

# Set seed for reproducibility
set.seed(299)

# Predict probabilities for positive class (ie high)
gbmA.prob <- predict(gbm.ada.fit, newdata = testing_data_auto, type = "prob")[, "high"]

# Compute and plot ROC
roc.gbmA <- roc(testing_data_auto$mpg_cat, gbmA.prob)
plot(roc.gbmA, col = 2)

# Test AUC
auc <- roc.gbmA$auc[1]
modelNames <- "Adaboost"
legend("bottomright", legend = paste0(modelNames, ": ", round(auc,3)),
col = 1:2, lwd = 2)

```

To find the test performance for this model, I computed the AUC (area under the ROC curve). Based on the plot, **AUC was 0.986**, which represents excellent classification performance for predicting mpg_cat on the test set. This would indicate that the adaboost model is fairly reliable in distinguishing between high and low fuel efficiency.


# References

Ridgeway, G. (2024, June 26). Generalized Boosted Models: A guide to the gbm package.

Nembrini, S., KÃ¶nig, I. R., & Wright, M. N. (2018). The revival of the Gini importance?. Bioinformatics, 34(21), 3711-3718.

